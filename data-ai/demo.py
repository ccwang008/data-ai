#!/usr/bin/env python3
"""
æ•°æ™ºä¸€ä½“åŒ–å¹³å°æ¼”ç¤ºè„šæœ¬
æ¼”ç¤ºå¹³å°çš„æ ¸å¿ƒåŠŸèƒ½ï¼šæ•°æ®æºç®¡ç†ã€æ•°æ®é‡‡é›†ã€ETLå¤„ç†ã€AIå»ºæ¨¡ç­‰
"""

import sys
import os
import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataAIPlatformDemo:
    """æ•°æ™ºä¸€ä½“åŒ–å¹³å°æ¼”ç¤ºç±»"""

    def __init__(self):
        self.platform_name = "æ•°æ™ºä¸€ä½“åŒ–å¹³å°"
        self.version = "1.0.0"
        self.data_sources = []
        self.datasets = []
        self.etl_workflows = []
        self.models = []

        # åˆ›å»ºæ¼”ç¤ºæ•°æ®ç›®å½•
        self.demo_dir = "demo_data"
        os.makedirs(self.demo_dir, exist_ok=True)

    def print_header(self, title: str):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "="*60)
        print(f"  {title}")
        print("="*60)

    def print_section(self, title: str):
        """æ‰“å°ç« èŠ‚"""
        print(f"\n{'-'*40}")
        print(f"  {title}")
        print(f"{'-'*40}")

    def demo_data_source_management(self):
        """æ¼”ç¤ºæ•°æ®æºç®¡ç†åŠŸèƒ½"""
        self.print_header("1. æ•°æ®æºç®¡ç†æ¨¡å—æ¼”ç¤º")

        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„æ•°æ®æº
        data_sources = [
            {
                "id": "ds_001",
                "name": "ä¸šåŠ¡æ•°æ®åº“",
                "type": "MySQL",
                "host": "mysql.example.com",
                "port": 3306,
                "database": "business_db",
                "status": "å·²è¿æ¥",
                "tables": 15,
                "last_sync": "2024-12-17 10:30:00"
            },
            {
                "id": "ds_002",
                "name": "ç”¨æˆ·è¡Œä¸ºæ—¥å¿—",
                "type": "Kafka",
                "brokers": ["kafka1:9092", "kafka2:9092"],
                "topics": ["user_actions", "page_views"],
                "status": "è¿è¡Œä¸­",
                "messages_per_sec": 1500,
                "last_sync": "å®æ—¶"
            },
            {
                "id": "ds_003",
                "name": "ç”¨æˆ·ç”»åƒæ•°æ®",
                "type": "MongoDB",
                "host": "mongo.example.com",
                "port": 27017,
                "database": "user_profiles",
                "status": "å·²è¿æ¥",
                "collections": 8,
                "documents": 1000000
            },
            {
                "id": "ds_004",
                "name": "APIæ•°æ®æ¥å£",
                "type": "REST API",
                "url": "https://api.example.com/v1",
                "status": "æ­£å¸¸",
                "endpoint_count": 25,
                "requests_per_day": 50000
            },
            {
                "id": "ds_005",
                "name": "äº‘å­˜å‚¨æ–‡ä»¶",
                "type": "MinIO",
                "bucket": "data-lake",
                "files": 5000,
                "total_size": "2.5TB",
                "status": "å·²è¿æ¥"
            }
        ]

        self.data_sources = data_sources
        self.print_section("å·²æ³¨å†Œçš„æ•°æ®æº")

        for ds in data_sources:
            print(f"ğŸ“Š {ds['name']} ({ds['type']})")
            print(f"   çŠ¶æ€: {ds['status']}")
            for key, value in ds.items():
                if key not in ['id', 'name', 'type', 'status']:
                    print(f"   {key}: {value}")
            print()

        # æ¼”ç¤ºè¿æ¥æµ‹è¯•
        self.print_section("æ•°æ®æºè¿æ¥æµ‹è¯•")
        for ds in data_sources:
            print(f"æµ‹è¯•è¿æ¥: {ds['name']} - {'âœ… æˆåŠŸ' if ds['status'] in ['å·²è¿æ¥', 'è¿è¡Œä¸­', 'æ­£å¸¸'] else 'âŒ å¤±è´¥'}")

    def demo_data_collection(self):
        """æ¼”ç¤ºæ•°æ®é‡‡é›†åŠŸèƒ½"""
        self.print_header("2. æ•°æ®é‡‡é›†æ¨¡å—æ¼”ç¤º")

        # æ¨¡æ‹Ÿæ•°æ®é‡‡é›†ä»»åŠ¡
        collection_tasks = [
            {
                "task_id": "task_001",
                "name": "ç”¨æˆ·è¡Œä¸ºæ•°æ®é‡‡é›†",
                "source": "ç”¨æˆ·è¡Œä¸ºæ—¥å¿—",
                "collection_type": "å®æ—¶æµé‡‡é›†",
                "status": "è¿è¡Œä¸­",
                "records_collected": 1500000,
                "start_time": "2024-12-01 00:00:00",
                "collection_rate": "1500æ¡/ç§’"
            },
            {
                "task_id": "task_002",
                "name": "ç”µå•†å•†å“æ•°æ®çˆ¬å–",
                "source": "å¤šä¸ªç”µå•†ç½‘ç«™",
                "collection_type": "ç½‘ç»œçˆ¬è™«",
                "status": "å·²å®Œæˆ",
                "records_collected": 50000,
                "start_time": "2024-12-15 08:00:00",
                "end_time": "2024-12-15 18:00:00"
            },
            {
                "task_id": "task_003",
                "name": "IoTè®¾å¤‡æ•°æ®é‡‡é›†",
                "source": "ä¼ æ„Ÿå™¨ç½‘ç»œ",
                "collection_type": "IoTæ•°æ®é‡‡é›†",
                "status": "è¿è¡Œä¸­",
                "devices": 100,
                "data_points": 2400000,
                "collection_rate": "1000ç‚¹/ç§’"
            }
        ]

        self.print_section("æ•°æ®é‡‡é›†ä»»åŠ¡çŠ¶æ€")
        for task in collection_tasks:
            print(f"ğŸ”„ {task['name']}")
            print(f"   é‡‡é›†ç±»å‹: {task['collection_type']}")
            print(f"   çŠ¶æ€: {task['status']}")
            print(f"   å·²é‡‡é›†è®°å½•: {task.get('records_collected', 0):,}")
            print(f"   é‡‡é›†é€Ÿç‡: {task.get('collection_rate', 'N/A')}")
            print()

    def demo_dataset_management(self):
        """æ¼”ç¤ºæ•°æ®é›†ç®¡ç†åŠŸèƒ½"""
        self.print_header("3. æ•°æ®é›†ç®¡ç†æ¨¡å—æ¼”ç¤º")

        # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
        datasets = [
            {
                "id": "dataset_001",
                "name": "ç”¨æˆ·è¡Œä¸ºæ•°æ®é›†",
                "type": "ç»“æ„åŒ–æ•°æ®",
                "source": "ä¸šåŠ¡æ•°æ®åº“",
                "records": 1000000,
                "size": "500MB",
                "columns": 25,
                "created_date": "2024-12-01",
                "last_updated": "2024-12-17",
                "tags": ["ç”¨æˆ·è¡Œä¸º", "å®æ—¶", "ç»“æ„åŒ–"],
                "version": "v1.2.0",
                "quality_score": 95.2
            },
            {
                "id": "dataset_002",
                "name": "å•†å“æ¨èè®­ç»ƒé›†",
                "type": "è®­ç»ƒæ•°æ®",
                "source": "ç”¨æˆ·è¡Œä¸ºæ—¥å¿—",
                "records": 500000,
                "size": "2GB",
                "columns": 18,
                "created_date": "2024-12-10",
                "last_updated": "2024-12-15",
                "tags": ["æ¨è", "æœºå™¨å­¦ä¹ ", "è®­ç»ƒé›†"],
                "version": "v2.1.0",
                "quality_score": 98.5
            },
            {
                "id": "dataset_003",
                "name": "ç”¨æˆ·è¯„è®ºæƒ…æ„Ÿæ•°æ®",
                "type": "æ–‡æœ¬æ•°æ®",
                "source": "ç”¨æˆ·è¯„è®º",
                "records": 200000,
                "size": "1.5GB",
                "created_date": "2024-12-05",
                "tags": ["æ–‡æœ¬", "æƒ…æ„Ÿåˆ†æ", "NLP"],
                "version": "v1.0.0",
                "quality_score": 92.8
            }
        ]

        self.datasets = datasets
        self.print_section("æ•°æ®é›†æ¦‚è§ˆ")

        for dataset in datasets:
            print(f"ğŸ“‹ {dataset['name']}")
            print(f"   ç±»å‹: {dataset['type']}")
            print(f"   è®°å½•æ•°: {dataset['records']:,}")
            print(f"   å¤§å°: {dataset['size']}")
            print(f"   æ ‡ç­¾: {', '.join(dataset['tags'])}")
            print(f"   è´¨é‡åˆ†æ•°: {dataset['quality_score']}")
            print()

        # æ¼”ç¤ºæ•°æ®é¢„è§ˆ
        self.print_section("æ•°æ®é¢„è§ˆç¤ºä¾‹")
        self._create_sample_data()

    def _create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        # ç”Ÿæˆç¤ºä¾‹ç”¨æˆ·è¡Œä¸ºæ•°æ®
        np.random.seed(42)
        n_records = 1000

        sample_data = {
            'user_id': np.random.randint(1, 10000, n_records),
            'action': np.random.choice(['view', 'click', 'purchase', 'cart'], n_records, p=[0.5, 0.3, 0.1, 0.1]),
            'page': np.random.choice(['home', 'product', 'category', 'search'], n_records),
            'timestamp': pd.date_range('2024-12-17', periods=n_records, freq='1min'),
            'duration': np.random.randint(5, 300, n_records),
            'device': np.random.choice(['mobile', 'desktop', 'tablet'], n_records)
        }

        df = pd.DataFrame(sample_data)

        print("ç¤ºä¾‹æ•°æ®é¢„è§ˆ (å‰5æ¡è®°å½•):")
        print(df.head().to_string(index=False))
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(df.describe())

        # ä¿å­˜ç¤ºä¾‹æ•°æ®
        df.to_csv(f"{self.demo_dir}/user_behavior_sample.csv", index=False)
        print(f"\nâœ… ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ° {self.demo_dir}/user_behavior_sample.csv")

    def demo_etl_workflow(self):
        """æ¼”ç¤ºETLå·¥ä½œæµåŠŸèƒ½"""
        self.print_header("4. å¯è§†åŒ–ETLæ¨¡å—æ¼”ç¤º")

        # æ¨¡æ‹ŸETLå·¥ä½œæµ
        etl_workflows = [
            {
                "workflow_id": "etl_001",
                "name": "ç”¨æˆ·è¡Œä¸ºæ•°æ®æ¸…æ´—",
                "status": "è¿è¡Œä¸­",
                "nodes": 8,
                "schedule": "æ¯30åˆ†é’Ÿ",
                "last_run": "2024-12-17 10:30:00",
                "next_run": "2024-12-17 11:00:00",
                "avg_duration": "5åˆ†é’Ÿ"
            },
            {
                "workflow_id": "etl_002",
                "name": "å®æ—¶æ¨èç‰¹å¾è®¡ç®—",
                "status": "è¿è¡Œä¸­",
                "nodes": 12,
                "schedule": "å®æ—¶æµ",
                "throughput": "5000æ¡/ç§’",
                "latency": "<100ms"
            },
            {
                "workflow_id": "etl_003",
                "name": "æ—¥æŠ¥æ•°æ®èšåˆ",
                "status": "å·²å®Œæˆ",
                "nodes": 6,
                "schedule": "æ¯å¤©02:00",
                "last_run": "2024-12-17 02:00:00",
                "duration": "15åˆ†é’Ÿ"
            }
        ]

        self.etl_workflows = etl_workflows
        self.print_section("ETLå·¥ä½œæµåˆ—è¡¨")

        for workflow in etl_workflows:
            print(f"ğŸ”„ {workflow['name']}")
            print(f"   çŠ¶æ€: {workflow['status']}")
            print(f"   èŠ‚ç‚¹æ•°: {workflow['nodes']}")
            print(f"   è°ƒåº¦: {workflow['schedule']}")
            if workflow['status'] == 'è¿è¡Œä¸­':
                print(f"   ä¸‹æ¬¡è¿è¡Œ: {workflow.get('next_run', 'N/A')}")
            else:
                print(f"   ä¸Šæ¬¡è¿è¡Œ: {workflow.get('last_run', 'N/A')}")
            print()

        # æ¼”ç¤ºETLèŠ‚ç‚¹ç±»å‹
        self.print_section("ETLèŠ‚ç‚¹ç±»å‹")
        node_types = [
            {"type": "è¾“å…¥èŠ‚ç‚¹", "examples": ["æ•°æ®åº“è¯»å–", "æ–‡ä»¶è¯»å–", "APIè¯»å–"]},
            {"type": "è½¬æ¢èŠ‚ç‚¹", "examples": ["æ•°æ®æ¸…æ´—", "å­—æ®µæ˜ å°„", "æ•°æ®è¿‡æ»¤"]},
            {"type": "èšåˆèŠ‚ç‚¹", "examples": ["åˆ†ç»„èšåˆ", "çª—å£èšåˆ", "è¿æ¥åˆå¹¶"]},
            {"type": "è¾“å‡ºèŠ‚ç‚¹", "examples": ["æ•°æ®åº“å†™å…¥", "æ–‡ä»¶è¾“å‡º", "æ¶ˆæ¯æ¨é€"]},
            {"type": "AIèŠ‚ç‚¹", "examples": ["æ¨¡å‹é¢„æµ‹", "ç‰¹å¾æå–", "å¼‚å¸¸æ£€æµ‹"]}
        ]

        for node_type in node_types:
            print(f"ğŸ”§ {node_type['type']}: {', '.join(node_type['examples'])}")

    def demo_ai_modeling(self):
        """æ¼”ç¤ºAIå»ºæ¨¡åŠŸèƒ½"""
        self.print_header("5. AIå»ºæ¨¡å¹³å°æ¼”ç¤º")

        # æ¨¡æ‹Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        models = [
            {
                "model_id": "model_001",
                "name": "ç”¨æˆ·æµå¤±é¢„æµ‹æ¨¡å‹",
                "type": "åˆ†ç±»æ¨¡å‹",
                "algorithm": "XGBoost",
                "status": "å·²éƒ¨ç½²",
                "accuracy": 0.92,
                "precision": 0.89,
                "recall": 0.94,
                "auc": 0.95,
                "training_data": "ç”¨æˆ·è¡Œä¸ºæ•°æ®é›†",
                "version": "v2.1.0",
                "deploy_date": "2024-12-10"
            },
            {
                "model_id": "model_002",
                "name": "å•†å“æ¨èæ¨¡å‹",
                "type": "æ¨èç³»ç»Ÿ",
                "algorithm": "DeepFM",
                "status": "å·²éƒ¨ç½²",
                "precision@10": 0.78,
                "recall@10": 0.65,
                "ndcg@10": 0.72,
                "training_data": "å•†å“æ¨èè®­ç»ƒé›†",
                "version": "v3.0.1",
                "deploy_date": "2024-12-08"
            },
            {
                "model_id": "model_003",
                "name": "è¯„è®ºæƒ…æ„Ÿåˆ†ææ¨¡å‹",
                "type": "NLPæ¨¡å‹",
                "algorithm": "BERT",
                "status": "è®­ç»ƒä¸­",
                "accuracy": 0.88,
                "f1_score": 0.87,
                "training_progress": "85%",
                "training_data": "ç”¨æˆ·è¯„è®ºæƒ…æ„Ÿæ•°æ®",
                "version": "v1.0.0"
            }
        ]

        self.models = models
        self.print_section("æœºå™¨å­¦ä¹ æ¨¡å‹")

        for model in models:
            print(f"ğŸ¤– {model['name']}")
            print(f"   ç±»å‹: {model['type']}")
            print(f"   ç®—æ³•: {model['algorithm']}")
            print(f"   çŠ¶æ€: {model['status']}")
            print(f"   ç‰ˆæœ¬: {model['version']}")

            # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
            metrics = [k for k in model.keys() if k in ['accuracy', 'precision', 'recall', 'auc', 'f1_score', 'precision@10', 'recall@10', 'ndcg@10']]
            if metrics:
                print("   æ€§èƒ½æŒ‡æ ‡:")
                for metric in metrics[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæŒ‡æ ‡
                    print(f"     {metric}: {model[metric]:.3f}")

            if model['status'] == 'è®­ç»ƒä¸­':
                print(f"   è®­ç»ƒè¿›åº¦: {model['training_progress']}")
            print()

    def demo_agent_platform(self):
        """æ¼”ç¤ºæ™ºèƒ½ä½“å¹³å°åŠŸèƒ½"""
        self.print_header("6. æ™ºèƒ½ä½“å¹³å°æ¼”ç¤º")

        # æ¨¡æ‹Ÿæ™ºèƒ½ä½“
        agents = [
            {
                "agent_id": "agent_001",
                "name": "æ•°æ®åˆ†æåŠ©æ‰‹",
                "type": "æ•°æ®åˆ†æAgent",
                "status": "è¿è¡Œä¸­",
                "capabilities": ["æ•°æ®æ¢ç´¢", "ç»Ÿè®¡åˆ†æ", "å¯è§†åŒ–ç”Ÿæˆ"],
                "tools": ["pandas", "matplotlib", "sql"],
                "last_activity": "2024-12-17 10:45:00",
                "tasks_completed": 156
            },
            {
                "agent_id": "agent_002",
                "name": "æ™ºèƒ½å®¢æœæœºå™¨äºº",
                "type": "å¯¹è¯Agent",
                "status": "è¿è¡Œä¸­",
                "capabilities": ["é—®ç­”å¯¹è¯", "é—®é¢˜è§£å†³", "å·¥å•åˆ›å»º"],
                "tools": ["çŸ¥è¯†åº“", "å·¥å•ç³»ç»Ÿ", "NLPæ¨¡å‹"],
                "last_activity": "2024-12-17 10:52:00",
                "tasks_completed": 2341
            },
            {
                "agent_id": "agent_003",
                "name": "æ•°æ®è´¨é‡ç›‘æ§Agent",
                "type": "ç›‘æ§Agent",
                "status": "è¿è¡Œä¸­",
                "capabilities": ["è´¨é‡æ£€æŸ¥", "å¼‚å¸¸æ£€æµ‹", "å‘Šè­¦é€šçŸ¥"],
                "tools": ["æ•°æ®è´¨é‡è§„åˆ™", "ç›‘æ§ç³»ç»Ÿ", "é€šçŸ¥æœåŠ¡"],
                "last_activity": "2024-12-17 10:48:00",
                "tasks_completed": 89
            }
        ]

        self.print_section("æ™ºèƒ½ä½“åˆ—è¡¨")

        for agent in agents:
            print(f"ğŸ¤– {agent['name']}")
            print(f"   ç±»å‹: {agent['type']}")
            print(f"   çŠ¶æ€: {agent['status']}")
            print(f"   èƒ½åŠ›: {', '.join(agent['capabilities'])}")
            print(f"   å·²å®Œæˆä»»åŠ¡: {agent['tasks_completed']}")
            print()

    def demo_data_visualization(self):
        """æ¼”ç¤ºæ•°æ®å¯è§†åŒ–åŠŸèƒ½"""
        self.print_header("7. æ•°æ®å¯è§†åŒ–æ¨¡å—æ¼”ç¤º")

        # æ¨¡æ‹Ÿä»ªè¡¨æ¿
        dashboards = [
            {
                "dashboard_id": "dash_001",
                "name": "ä¸šåŠ¡è¿è¥é©¾é©¶èˆ±",
                "description": "å®æ—¶ç›‘æ§ä¸šåŠ¡æ ¸å¿ƒæŒ‡æ ‡",
                "charts": 8,
                "refresh_rate": "5åˆ†é’Ÿ",
                "viewers": 45,
                "last_viewed": "2024-12-17 10:50:00"
            },
            {
                "dashboard_id": "dash_002",
                "name": "ç”¨æˆ·è¡Œä¸ºåˆ†æ",
                "description": "æ·±å…¥åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼",
                "charts": 12,
                "refresh_rate": "1å°æ—¶",
                "viewers": 28,
                "last_viewed": "2024-12-17 09:30:00"
            },
            {
                "dashboard_id": "dash_003",
                "name": "AIæ¨¡å‹ç›‘æ§",
                "description": "ç›‘æ§AIæ¨¡å‹æ€§èƒ½å’Œé¢„æµ‹",
                "charts": 6,
                "refresh_rate": "å®æ—¶",
                "viewers": 15,
                "last_viewed": "2024-12-17 10:55:00"
            }
        ]

        self.print_section("æ•°æ®ä»ªè¡¨æ¿")

        for dashboard in dashboards:
            print(f"ğŸ“Š {dashboard['name']}")
            print(f"   æè¿°: {dashboard['description']}")
            print(f"   å›¾è¡¨æ•°: {dashboard['charts']}")
            print(f"   åˆ·æ–°é¢‘ç‡: {dashboard['refresh_rate']}")
            print(f"   ä»Šæ—¥è®¿å®¢: {dashboard['viewers']}")
            print()

        # ç”Ÿæˆç¤ºä¾‹å›¾è¡¨æ•°æ®
        self.print_section("ç¤ºä¾‹å›¾è¡¨æ•°æ®")
        self._generate_chart_data()

    def _generate_chart_data(self):
        """ç”Ÿæˆç¤ºä¾‹å›¾è¡¨æ•°æ®"""
        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        dates = pd.date_range('2024-12-01', '2024-12-17', freq='D')

        # ç”¨æˆ·æ´»è·ƒåº¦è¶‹åŠ¿
        active_users = np.random.randint(10000, 15000, len(dates))
        new_users = np.random.randint(500, 1000, len(dates))

        print("ğŸ“ˆ ç”¨æˆ·æ´»è·ƒåº¦è¶‹åŠ¿ (æœ€è¿‘7å¤©)")
        for i in range(-7, 0):
            date_str = dates[i].strftime('%Y-%m-%d')
            print(f"   {date_str}: æ´»è·ƒç”¨æˆ· {active_users[i]:,}, æ–°å¢ç”¨æˆ· {new_users[i]:,}")

        # è¡Œä¸ºåˆ†å¸ƒé¥¼å›¾æ•°æ®
        actions = ['view', 'click', 'purchase', 'cart', 'search']
        action_counts = [45000, 28000, 3500, 8000, 12000]

        print(f"\nğŸ¥§ ç”¨æˆ·è¡Œä¸ºåˆ†å¸ƒ")
        for action, count in zip(actions, action_counts):
            percentage = count / sum(action_counts) * 100
            print(f"   {action}: {count:,} ({percentage:.1f}%)")

    def demo_system_monitoring(self):
        """æ¼”ç¤ºç³»ç»Ÿç›‘æ§åŠŸèƒ½"""
        self.print_header("8. ç³»ç»Ÿç›‘æ§ä¸ç®¡ç†æ¨¡å—æ¼”ç¤º")

        # ç³»ç»ŸçŠ¶æ€
        system_status = {
            "å¹³å°æ•´ä½“çŠ¶æ€": "æ­£å¸¸",
            "æœåŠ¡å¯ç”¨æ€§": "99.95%",
            "å“åº”æ—¶é—´": "125ms",
            "æ•°æ®å¤„ç†é‡": "2.5TB/å¤©",
            "æ´»è·ƒç”¨æˆ·": "1,250",
            "åœ¨çº¿æ•°æ®æº": "15/15",
            "è¿è¡ŒETLä»»åŠ¡": "23/25",
            "å·²éƒ¨ç½²æ¨¡å‹": "12",
            "æ´»è·ƒAgent": "8"
        }

        self.print_section("ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ")
        for metric, value in system_status.items():
            print(f"ğŸ“Š {metric}: {value}")

        # æœåŠ¡çŠ¶æ€
        services = [
            {"name": "APIç½‘å…³", "status": "æ­£å¸¸", "cpu": "45%", "memory": "2.1GB", "requests": "1.2k/ç§’"},
            {"name": "æ•°æ®å¤„ç†å¼•æ“", "status": "æ­£å¸¸", "cpu": "68%", "memory": "4.5GB", "throughput": "50MB/ç§’"},
            {"name": "AIæ¨ç†æœåŠ¡", "status": "æ­£å¸¸", "cpu": "52%", "memory": "3.8GB", "predictions": "800/ç§’"},
            {"name": "æ¶ˆæ¯é˜Ÿåˆ—", "status": "æ­£å¸¸", "cpu": "23%", "memory": "1.2GB", "messages": "15k/ç§’"},
            {"name": "ç¼“å­˜æœåŠ¡", "status": "æ­£å¸¸", "cpu": "18%", "memory": "2.8GB", "hit_rate": "94.5%"}
        ]

        self.print_section("å¾®æœåŠ¡çŠ¶æ€")
        for service in services:
            print(f"ğŸ”§ {service['name']}")
            print(f"   çŠ¶æ€: {service['status']}")
            print(f"   CPUä½¿ç”¨ç‡: {service['cpu']}")
            print(f"   å†…å­˜ä½¿ç”¨: {service['memory']}")
            for key, value in service.items():
                if key not in ['name', 'status', 'cpu', 'memory']:
                    print(f"   {key}: {value}")
            print()

    def demo_data_security(self):
        """æ¼”ç¤ºæ•°æ®å®‰å…¨åŠŸèƒ½"""
        self.print_header("9. æ•°æ®å®‰å…¨ä¸æƒé™ç®¡ç†æ¼”ç¤º")

        # å®‰å…¨ç»Ÿè®¡
        security_stats = {
            "åŠ å¯†æ•°æ®æº": "15/15",
            "æ•°æ®è„±æ•è§„åˆ™": "128",
            "è®¿é—®æ§åˆ¶ç­–ç•¥": "45",
            "ä»Šæ—¥ç™»å½•ç”¨æˆ·": "238",
            "æƒé™å˜æ›´è®°å½•": "12",
            "å®‰å…¨å‘Šè­¦": "0",
            "æ•°æ®å®¡è®¡æ—¥å¿—": "1,258,450"
        }

        self.print_section("å®‰å…¨çŠ¶æ€")
        for metric, value in security_stats.items():
            print(f"ğŸ”’ {metric}: {value}")

        # æƒé™ç®¡ç†
        permissions = [
            {"role": "æ•°æ®åˆ†æå¸ˆ", "users": 45, "permissions": ["æ•°æ®æŸ¥çœ‹", "æŠ¥è¡¨åˆ›å»º", "æŸ¥è¯¢æ‰§è¡Œ"]},
            {"role": "æ•°æ®å·¥ç¨‹å¸ˆ", "users": 12, "permissions": ["æ•°æ®ç®¡ç†", "ETLè®¾è®¡", "ä»»åŠ¡è°ƒåº¦"]},
            {"role": "AIå·¥ç¨‹å¸ˆ", "users": 8, "permissions": ["æ¨¡å‹è®­ç»ƒ", "æ¨¡å‹éƒ¨ç½²", "é¢„æµ‹æœåŠ¡"]},
            {"role": "ç³»ç»Ÿç®¡ç†å‘˜", "users": 3, "permissions": ["ç³»ç»Ÿé…ç½®", "ç”¨æˆ·ç®¡ç†", "æƒé™åˆ†é…"]},
            {"role": "ä¸šåŠ¡ç”¨æˆ·", "users": 170, "permissions": ["æŠ¥è¡¨æŸ¥çœ‹", "æ•°æ®å¯¼å‡º"]}
        ]

        self.print_section("è§’è‰²æƒé™ç®¡ç†")
        for role in permissions:
            print(f"ğŸ‘¥ {role['role']} ({role['users']}äºº)")
            print(f"   æƒé™: {', '.join(role['permissions'])}")
            print()

    def demo_integration_capabilities(self):
        """æ¼”ç¤ºé›†æˆèƒ½åŠ›"""
        self.print_header("10. å¹³å°é›†æˆèƒ½åŠ›æ¼”ç¤º")

        # é›†æˆè¿æ¥å™¨
        connectors = {
            "æ•°æ®åº“è¿æ¥å™¨": ["MySQL", "PostgreSQL", "Oracle", "SQL Server", "MongoDB", "Redis"],
            "æ¶ˆæ¯é˜Ÿåˆ—": ["Kafka", "RabbitMQ", "Pulsar", "ActiveMQ"],
            "äº‘å¹³å°": ["AWS S3", "é˜¿é‡Œäº‘OSS", "è…¾è®¯äº‘COS", "åä¸ºäº‘OBS"],
            "APIæ¡†æ¶": ["REST API", "GraphQL", "gRPC", "WebSocket"],
            "å¤§æ•°æ®å¹³å°": ["Hadoop", "Spark", "Flink", "ClickHouse"],
            "æœºå™¨å­¦ä¹ ": ["TensorFlow", "PyTorch", "Scikit-learn", "MLflow"]
        }

        self.print_section("æ”¯æŒçš„é›†æˆè¿æ¥å™¨")
        for category, items in connectors.items():
            print(f"ğŸ”Œ {category}: {', '.join(items)}")

        # å¼€æ”¾API
        self.print_section("å¼€æ”¾APIèƒ½åŠ›")
        apis = [
            {"name": "æ•°æ®æºç®¡ç†API", "endpoints": 15, "requests_per_day": "50ä¸‡"},
            {"name": "æ•°æ®æŸ¥è¯¢API", "endpoints": 12, "requests_per_day": "200ä¸‡"},
            {"name": "ETLè°ƒåº¦API", "endpoints": 8, "requests_per_day": "10ä¸‡"},
            {"name": "AIé¢„æµ‹API", "endpoints": 6, "requests_per_day": "500ä¸‡"},
            {"name": "Agenté€šä¿¡API", "endpoints": 10, "requests_per_day": "100ä¸‡"}
        ]

        for api in apis:
            print(f"ğŸ”— {api['name']}")
            print(f"   APIç«¯ç‚¹æ•°: {api['endpoints']}")
            print(f"   æ—¥è°ƒç”¨é‡: {api['requests_per_day']}")
            print()

    def generate_summary_report(self):
        """ç”Ÿæˆå¹³å°æ€»ç»“æŠ¥å‘Š"""
        self.print_header("æ•°æ™ºä¸€ä½“åŒ–å¹³å° - æ¼”ç¤ºæ€»ç»“æŠ¥å‘Š")

        summary = {
            "æ•°æ®æºæ€»æ•°": len(self.data_sources),
            "æ•°æ®é›†æ€»æ•°": len(self.datasets),
            "ETLå·¥ä½œæµæ•°": len(self.etl_workflows),
            "AIæ¨¡å‹æ•°": len(self.models),
            "å¹³å°ç‰ˆæœ¬": self.version,
            "æ¼”ç¤ºæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        self.print_section("å¹³å°èƒ½åŠ›æ¦‚è§ˆ")
        for key, value in summary.items():
            print(f"ğŸ“Š {key}: {value}")

        self.print_section("æ ¸å¿ƒä¼˜åŠ¿")
        advantages = [
            "âœ… ç»Ÿä¸€çš„æ•°æ®ä¸­å°æ¶æ„ï¼Œæ”¯æŒå¤šç§æ•°æ®æºæ¥å…¥",
            "âœ… å¯è§†åŒ–ETLå·¥å…·ï¼Œé™ä½æ•°æ®å¤„ç†æŠ€æœ¯é—¨æ§›",
            "âœ… å®Œæ•´çš„AIå»ºæ¨¡å¹³å°ï¼Œæ”¯æŒç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ æµç¨‹",
            "âœ… æ™ºèƒ½ä½“å¹³å°ï¼Œæ”¯æŒå¤šAgentåä½œå’Œå·¥ä½œæµç¼–æ’",
            "âœ… ä¸°å¯Œçš„å¯è§†åŒ–ç»„ä»¶ï¼Œæ”¯æŒå¤§å±å’Œä»ªè¡¨æ¿å®šåˆ¶",
            "âœ… ä¼ä¸šçº§å®‰å…¨ä¿éšœï¼Œå®Œå–„çš„æƒé™ç®¡ç†å’Œæ•°æ®åŠ å¯†",
            "âœ… é«˜å¯ç”¨æ¶æ„è®¾è®¡ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•å’Œæ•…éšœæ¢å¤",
            "âœ… å¼€æ”¾çš„APIç”Ÿæ€ï¼Œæ”¯æŒç¬¬ä¸‰æ–¹ç³»ç»Ÿé›†æˆ"
        ]

        for advantage in advantages:
            print(f"   {advantage}")

        self.print_section("åº”ç”¨åœºæ™¯")
        scenarios = [
            "ğŸ¢ ä¼ä¸šæ•°æ®ä¸­å°å»ºè®¾ - ç»Ÿä¸€ç®¡ç†ä¼ä¸šæ•°æ®èµ„äº§",
            "ğŸ“Š å®æ—¶æ•°æ®åˆ†æå¹³å° - æ”¯æŒå®æ—¶å†³ç­–å’Œä¸šåŠ¡æ´å¯Ÿ",
            "ğŸ¤– AIæ¨¡å‹å·¥å‚ - è§„æ¨¡åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘ä¸éƒ¨ç½²",
            "ğŸ“ˆ æ™ºèƒ½è¿è¥ç³»ç»Ÿ - è‡ªåŠ¨åŒ–ä¸šåŠ¡ç›‘æ§å’Œä¼˜åŒ–",
            "ğŸ” ç”¨æˆ·ç”»åƒå¹³å° - æ·±åº¦ç†è§£ç”¨æˆ·è¡Œä¸ºå’Œåå¥½",
            "ğŸ¯ ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ - æå‡ç”¨æˆ·ä½“éªŒå’Œè½¬åŒ–ç‡",
            "ğŸ”’ æ•°æ®æ²»ç†å¹³å° - ç¡®ä¿æ•°æ®è´¨é‡å’Œåˆè§„æ€§"
        ]

        for scenario in scenarios:
            print(f"   {scenario}")

        print(f"\n{'='*60}")
        print("  ğŸ‰ æ•°æ™ºä¸€ä½“åŒ–å¹³å°æ¼”ç¤ºå®Œæˆï¼")
        print("  ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ•°æ®æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼ŒåŠ©åŠ›ä¼ä¸šæ•°å­—åŒ–è½¬å‹")
        print("="*60)

    def run_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print(f"ğŸš€ æ¬¢è¿ä½¿ç”¨ {self.platform_name} v{self.version}")
        print("æ­£åœ¨å¯åŠ¨å¹³å°æ¼”ç¤º...\n")

        try:
            self.demo_data_source_management()
            self.demo_data_collection()
            self.demo_dataset_management()
            self.demo_etl_workflow()
            self.demo_ai_modeling()
            self.demo_agent_platform()
            self.demo_data_visualization()
            self.demo_system_monitoring()
            self.demo_data_security()
            self.demo_integration_capabilities()
            self.generate_summary_report()

        except Exception as e:
            logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        else:
            print(f"\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    demo = DataAIPlatformDemo()
    demo.run_demo()

    # ç”Ÿæˆæ¼”ç¤ºæŠ¥å‘Šæ–‡ä»¶
    report_file = "demo_report.json"
    report_data = {
        "platform_name": demo.platform_name,
        "version": demo.version,
        "demo_time": datetime.now().isoformat(),
        "data_sources_count": len(demo.data_sources),
        "datasets_count": len(demo.datasets),
        "etl_workflows_count": len(demo.etl_workflows),
        "models_count": len(demo.models),
        "summary": "æ•°æ™ºä¸€ä½“åŒ–å¹³å°æ¼”ç¤ºæˆåŠŸå®Œæˆ"
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ æ¼”ç¤ºæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    main()